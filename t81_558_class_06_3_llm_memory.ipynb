{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2Z2JH6cR3bM"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_06_3_llm_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "On4gWCWuR3bN"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 6: ChatGPT and Large Language Models**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QVOz9xNR3bO"
   },
   "source": [
    "# Module 6 Material\n",
    "\n",
    "* Part 6.1: Introduction to Transformers [[Video]](https://www.youtube.com/watch?v=mn6r5PYJcu0&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_1_transformers.ipynb)\n",
    "* Part 6.2: Accessing the ChatGPT API [[Video]](https://www.youtube.com/watch?v=tcdscXl4o5w&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_2_chat_gpt.ipynb)\n",
    "* **Part 6.3: LLM Memory** [[Video]](https://www.youtube.com/watch?v=oGQ3TQx1Qs8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_3_llm_memory.ipynb)\n",
    "* Part 6.4: Introduction to Embeddings [[Video]](https://www.youtube.com/watch?v=e6kcs9Uj_ps&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_4_embedding.ipynb)\n",
    "* Part 6.5: Prompt Engineering [[Video]](https://www.youtube.com/watch?v=miTpIDR7k6c&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_5_prompt_engineering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLI2X9tnR3bO"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow.\n",
    "  Running the following code will map your GDrive to ```/content/drive```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjyDLCCuR3bO",
    "outputId": "5193c3bc-fa81-4450-fce0-06fd402e8672"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVpAB-sNR3bP"
   },
   "source": [
    "# Part 6.3: LLM Memory\n",
    "\n",
    "Human minds have both long-term and short-term memory. Long-term memory is what the human has learned throughout their lifetime. Short-term memory is what a human has only recently discovered in the last minute or so. For humans, learning is converting short-term memory into long-term memory that we will retain.\n",
    "\n",
    "This process works somewhat differently for a LLM. Long-term memory was the weight of the neural network when it was initially trained or finetuned. Short-term memory is additional information that we wish the LLM to retain from previous prompts. For example, if the first prompt is \"My name is Jeff\", the LLM will likely tell you hello and repeat your name. However, the LLM will not know the answer if the second prompt is \"What is my name.\" without adding a memory component.\n",
    "\n",
    "These memory objects, which LangChain provides, provide a sort of short-term memory. It is important to note that these objects are not affecting the long-term memory of the LLM, and once you discard the memory object, the LLM will forget. Additionally, the memory object can only hold so much information; newer information may replace older information once it is filled.\n",
    "\n",
    "One important point to remember is that LLM's only have their input prompt. To provide such memory, these objects are appending anything we wish the LLM to remember to the input prompt. This section will see two ways to augment the prompt with previous information: a buffer and a summary. The buffer prepends a script of the last conversation up to this point. The summary approach keeps a consistently updated summary paragraph of the conversation.\n",
    "\n",
    "## Install LangChain\n",
    "\n",
    "Just as we did previously, we mustinstall LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_pz8MuY84Qh",
    "outputId": "9320d45e-1fef-46b9-99fb-941c0577328b"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE0FlZ_eR3bP"
   },
   "source": [
    "Smiliarly, we must also specify our API key and model touse. Refer to Module 6.2 for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VucO3HSMoJkz"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "\n",
    "# Your OpenAI API key\n",
    "# If you are in my class at WUSTL, get this key from the Assignment 6 description in Canvas.\n",
    "\n",
    "OPENAI_KEY = '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'\n",
    "\n",
    "# This is the model you will generally use for this class\n",
    "LLM_MODEL = 'gpt-3.5-turbo-instruct'\n",
    "\n",
    "# Chat Model: LLM_MODEL = 'gpt-3.5-turbo-1106'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0McrknBXgxG"
   },
   "source": [
    "Next, we create the LLM model, as we did before. We specify a higher temperature to encourage creativity in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0CRDXlUW0V5"
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenAI LLM (Language Learning Model) with your API key\n",
    "llm = OpenAI(openai_api_key=OPENAI_KEY, model=LLM_MODEL, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6kkALepWLi9"
   },
   "source": [
    "## Conversation Buffer Window Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aufxkOj1R3bP"
   },
   "source": [
    "The LangChain library includes a conversation object named **ConversationChain**; this object facilitates an ongoing conversation with an LLM. For any conversation object, you must also specify a memory. For this first example, we will use the **ConversationBufferWindowMemory** object. This object keeps a transcript of the most recent conversation to reference. This memory allows the conversation object to remember what you have asked or told it and its responses to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3_lzwcqermJ"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxg_SXT2R3bP"
   },
   "source": [
    "We can now have a conversation with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "Egh3beCVRpW5",
    "outputId": "c8de5cd3-b5e9-4ebb-a756-e4b64ecf6dae"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi, my name is Jeff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "gWi1o0tuYOR3",
    "outputId": "f610ce7e-dd4e-4701-dee1-fe21fcf0a268"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3u8kB-ccx2k"
   },
   "source": [
    "We can have a look at what the memory now contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LC8dFPqoaa7Z",
    "outputId": "3f84cbb8-de1d-42d9-f735-9a6aab420f87"
   },
   "outputs": [],
   "source": [
    "conversation.memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EJfbbLSYTnO"
   },
   "source": [
    "## Custom Conversation Bots\n",
    "\n",
    "You can define the prompt template for a conversationbot. This technique allows you to create a bot with a name and perform a specialized task. In this case, we created a bot named \"WashU Assistant\" that we designed to help students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BZAlBkBYajX"
   },
   "outputs": [],
   "source": [
    "# Now we can override it and set it to \"AI Assistant\"\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an\n",
    "AI to assist Washington University Students. The AI should stick to topics\n",
    "about Washington University. If the AI does not know the answer to a question,\n",
    "it should suggest the student speak to their advisor.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "WashU Assistant:\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferWindowMemory(ai_prefix=\"WashU Assistant\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7_y05_Pc8Be"
   },
   "source": [
    "We can now have a conversation with the WUSTL assistant bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "JBH_Zp0NYrW1",
    "outputId": "8b02cb8f-e96f-4657-fc98-a5bec5768a41"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Where is the bookstore?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "4LNHTxdMYu1h",
    "outputId": "36803406-2543-44b2-c047-256a168880dd"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is a nice quiet area to study?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "my3IvjLhY1eh",
    "outputId": "4385be9f-3214-4d30-eedf-36afe1300149"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Which of these is closest to the bookstore?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "-tx7V8d8Zlng",
    "outputId": "b55eaec5-5995-4f79-8069-0682841c32dd"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"What is the meaning of life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3kzuQqRc2Lu"
   },
   "source": [
    "We can have a look at what the memory now contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bwq-I9lYaZuP",
    "outputId": "0602d0b4-c527-4352-f40b-57314c253e07"
   },
   "outputs": [],
   "source": [
    "conversation.memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6Yjeri4Zv9x"
   },
   "source": [
    "## Conversation Summary Memory\n",
    "\n",
    "Now, let's look at using a slightly more complex type of memory, the ConversationSummaryMemory object. This type of memory creates a summary of the conversation over time. This memory can be helpful for condensing information from the conversation over time. Conversation summary memory summarizes the conversation and stores the current summary in memory. You can use this memory to inject the conversation summary so far into a prompt/chain. This memory is most useful for more extended conversations, where keeping the past message history in the prompt verbatim would take up too many tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wJsN0k_aMVS"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "4DEl67mQaPvp",
    "outputId": "3efb44c8-79cf-439f-d9d9-ef64cd0bc9df"
   },
   "outputs": [],
   "source": [
    "conversation.predict(input=\"I am a computer scientist, what do you do for a living?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG5LSIvgc4yJ"
   },
   "source": [
    "We can have a look at what the memory now contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTFOPjHjaSBL",
    "outputId": "72263729-e827-4330-93b7-d21085034c56"
   },
   "outputs": [],
   "source": [
    "conversation.memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
